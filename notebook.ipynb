{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb41801e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi!'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, We are on #5 MEMORY! Let's goooo\n",
    "\n",
    "# If you don't add 'memory' to chatbot, chatbot can't remember anything..\n",
    "# even following questions, chatbot dont understand if dont have memory => stateless\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "memory.save_context({\"input\": \"Hi!\"},{\"output\": \"How are you?\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# ConversationBufferMemory save all conversation. it's inefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dcd5e573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='1'),\n",
       "  AIMessage(content='1'),\n",
       "  HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4')]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    return_messages=True,\n",
    "    k=4\n",
    ")\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\":input}, {\"output\":output})\n",
    "\n",
    "add_message(1, 1)\n",
    "add_message(2, 2)\n",
    "add_message(3, 3)\n",
    "add_message(4, 4)\n",
    "\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5ea0ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='2'),\n",
       "  AIMessage(content='2'),\n",
       "  HumanMessage(content='3'),\n",
       "  AIMessage(content='3'),\n",
       "  HumanMessage(content='4'),\n",
       "  AIMessage(content='4'),\n",
       "  HumanMessage(content='5'),\n",
       "  AIMessage(content='5')]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(5,5)\n",
    "memory.load_memory_variables({})\n",
    "# ConversationBufferWindowMemory fix conversation size always. it's good to make efficient, but chatbot cant remember old messages.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3e35fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we use llm with memory. it costs.\n",
    "# ConversationSummaryMemory: summary of the conversation automatically\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "def get_history():\n",
    "    return memory.load_memory_variables({})\n",
    "\n",
    "add_message(\"Hi, My name is Sofia Kim. i live in South Korea\", \"Wow that is so cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "063cb580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'The human introduces herself as Sofia Kim from South Korea. The AI responds with enthusiasm about her location and name, expressing a desire to visit Korea because it is pretty.'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"Korea is pretty\", \"I wish to go there\")\n",
    "\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e48dfce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, My name is Sofia Kim. i live in Japan'),\n",
       "  AIMessage(content='Wow that is so cool')]}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ConversationSummaryBufferMemory = window buffer memory + buffer window memory\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=50,\n",
    "    return_messages=True,\n",
    ")\n",
    "# max_token_limit is max size before summery (original conversation max)\n",
    "\n",
    "add_message(\"Hi, My name is Sofia Kim. i live in Japan\", \"Wow that is so cool\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d38959f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi, My name is Sofia Kim. i live in Japan'),\n",
       "  AIMessage(content='Wow that is so cool'),\n",
       "  HumanMessage(content='Japan weather is warm'),\n",
       "  AIMessage(content='I wish I could go!')]}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"Japan weather is warm\", \"I wish I could go!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "71cf1f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces herself as Sofia Kim and mentions she lives in Japan.'),\n",
       "  AIMessage(content='Wow that is so cool'),\n",
       "  HumanMessage(content='Japan weather is warm'),\n",
       "  AIMessage(content='I wish I could go!'),\n",
       "  HumanMessage(content='How far Japan to Korea?'),\n",
       "  AIMessage(content='near')]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"How far Japan to Korea?\", \"near\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6526b101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human introduces herself as Sofia Kim and mentions she lives in Japan. The AI expresses admiration for this and Sofia mentions the warm weather in Japan, prompting the AI to express a desire to visit.'),\n",
       "  HumanMessage(content='How far Japan to Korea?'),\n",
       "  AIMessage(content='near'),\n",
       "  HumanMessage(content='Korea dish is so delicious'),\n",
       "  AIMessage(content='Oh I like Korean Food!')]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"Korea dish is so delicious\", \"Oh I like Korean Food!\")\n",
    "get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5ecb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationKGMemory : it choose/extract 'entity' from your conversation\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "memory = ConversationKGMemory(\n",
    "    llm=llm,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "add_message(\"Hi, My name is Sofia Kim. i live in Japan\", \"Wow that is so cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b37ec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Sofia Kim: Sofia Kim lives in Japan.')]}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"input\": \"who is Sofia Kim\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce331e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='On Sofia: Sofia likes Kimchi.')]}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_message(\"Sofia likes Kimchi\", \"Wow that is so cool\")\n",
    "memory.load_memory_variables({\"input\": \"What does she like\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06751e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    \n",
      "    Human:My name is Sofia Kim\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Sofia Kim\n",
      "AI: Hello Sofia Kim! How can I assist you today?\n",
      "    Human:I live in Seoul. it's beautiful city\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"That's wonderful to hear! Seoul is indeed a beautiful city with a rich history and vibrant culture. Is there anything specific you would like to know or discuss about Seoul?\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# we need 'space' for memory. that is 'chat_history'(variable)\n",
    "template = \"\"\"\n",
    "    You are a helpful AI talking to a human.\n",
    "\n",
    "    {chat_history}\n",
    "    Human:{question}\n",
    "    You:\n",
    "\"\"\"\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    prompt=PromptTemplate.from_template(template)\n",
    ")\n",
    "\n",
    "chain.predict(question=\"My name is Sofia Kim\")\n",
    "chain.predict(question=\"I live in Seoul. it's beautiful city\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3146999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "    You are a helpful AI talking to a human.\n",
      "\n",
      "    Human: My name is Sofia Kim\n",
      "AI: Hello Sofia Kim! How can I assist you today?\n",
      "Human: I live in Seoul. it's beautiful city\n",
      "AI: That's wonderful to hear! Seoul is indeed a beautiful city with a rich history and vibrant culture. Is there anything specific you would like to know or discuss about Seoul?\n",
      "    Human:What is my name?\n",
      "    You:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Sofia Kim.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b96a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a helpful AI talking to a human.\n",
      "Human: My name is Sofia Kim\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a helpful AI talking to a human.\n",
      "Human: My name is Sofia Kim\n",
      "AI: Nice to meet you, Sofia Kim! How can I assist you today?\n",
      "Human: I live in Seoul. it's beautiful city\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: you are a helpful AI talking to a human.\n",
      "Human: My name is Sofia Kim\n",
      "AI: Nice to meet you, Sofia Kim! How can I assist you today?\n",
      "Human: I live in Seoul. it's beautiful city\n",
      "AI: Seoul is indeed a beautiful city with a rich history and vibrant culture. Is there anything specific you enjoy about living in Seoul?\n",
      "Human: What is my name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Sofia Kim.'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# memory class can out two types. \n",
    "# One type is string, and One type is message\n",
    "memory.load_memory_variables({}) # you can see print just 'text'(string)\n",
    "\n",
    "# If you change output type to message, follow below parameter : return_message\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "\n",
    "chain.predict(question=\"My name is Sofia Kim\") # You can see print type 'message'\n",
    "chain.predict(question=\"I live in Seoul. it's beautiful city\")\n",
    "chain.predict(question=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7b8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Nice to meet you, Sofia! How can I assist you today?'\n",
      "content='Your name is Sofia.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"you are a helpful AI talking to a human.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "def load_memory(_): \n",
    "    # if you dont put any parameter, it will cause error. \n",
    "    # because it should given 'question'(=input). every memory class given input, and through out output. remember! \n",
    "    return memory.load_memory_variables({})[\"chat_history\"]\n",
    "\n",
    "chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm # you can use several functions in RunnablePassthrough class\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = chain.invoke({\n",
    "        \"question\": question\n",
    "    })\n",
    "    memory.save_context({\"input\": question},{\"output\": result.content})\n",
    "    print(result)\n",
    "\n",
    "invoke_chain(\"My name is Sofia\")\n",
    "invoke_chain(\"What is my name?\")\n",
    "\n",
    "# So far, we learned 3 ways to add 'memory' in 'prompt'\n",
    "# 1. using 'LLM chain'\n",
    "# 2. using 'Chat prompt template'\n",
    "# 3. using 'manual memory management' (it's not auto, but this way could be better and efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b29c9990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 963, which is longer than the specified 600\n",
      "Created a chunk of size 774, which is longer than the specified 600\n",
      "Created a chunk of size 954, which is longer than the specified 600\n",
      "Created a chunk of size 922, which is longer than the specified 600\n",
      "Created a chunk of size 1168, which is longer than the specified 600\n",
      "Created a chunk of size 821, which is longer than the specified 600\n",
      "Created a chunk of size 700, which is longer than the specified 600\n",
      "Created a chunk of size 745, which is longer than the specified 600\n",
      "Created a chunk of size 735, which is longer than the specified 600\n",
      "Created a chunk of size 1110, which is longer than the specified 600\n",
      "Created a chunk of size 991, which is longer than the specified 600\n",
      "Created a chunk of size 990, which is longer than the specified 600\n",
      "Created a chunk of size 1182, which is longer than the specified 600\n",
      "Created a chunk of size 1491, which is longer than the specified 600\n",
      "Created a chunk of size 1401, which is longer than the specified 600\n",
      "Created a chunk of size 1130, which is longer than the specified 600\n",
      "Created a chunk of size 1326, which is longer than the specified 600\n",
      "Created a chunk of size 1449, which is longer than the specified 600\n",
      "Created a chunk of size 1364, which is longer than the specified 600\n",
      "Created a chunk of size 999, which is longer than the specified 600\n",
      "Created a chunk of size 930, which is longer than the specified 600\n",
      "Created a chunk of size 1022, which is longer than the specified 600\n",
      "Created a chunk of size 1260, which is longer than the specified 600\n",
      "Created a chunk of size 795, which is longer than the specified 600\n",
      "Created a chunk of size 1293, which is longer than the specified 600\n",
      "Created a chunk of size 649, which is longer than the specified 600\n",
      "Created a chunk of size 963, which is longer than the specified 600\n",
      "Created a chunk of size 774, which is longer than the specified 600\n",
      "Created a chunk of size 954, which is longer than the specified 600\n",
      "Created a chunk of size 922, which is longer than the specified 600\n",
      "Created a chunk of size 1168, which is longer than the specified 600\n",
      "Created a chunk of size 821, which is longer than the specified 600\n",
      "Created a chunk of size 700, which is longer than the specified 600\n",
      "Created a chunk of size 745, which is longer than the specified 600\n",
      "Created a chunk of size 735, which is longer than the specified 600\n",
      "Created a chunk of size 1110, which is longer than the specified 600\n",
      "Created a chunk of size 991, which is longer than the specified 600\n",
      "Created a chunk of size 990, which is longer than the specified 600\n",
      "Created a chunk of size 1182, which is longer than the specified 600\n",
      "Created a chunk of size 1491, which is longer than the specified 600\n",
      "Created a chunk of size 1401, which is longer than the specified 600\n",
      "Created a chunk of size 1130, which is longer than the specified 600\n",
      "Created a chunk of size 1326, which is longer than the specified 600\n",
      "Created a chunk of size 1449, which is longer than the specified 600\n",
      "Created a chunk of size 1364, which is longer than the specified 600\n",
      "Created a chunk of size 999, which is longer than the specified 600\n",
      "Created a chunk of size 930, which is longer than the specified 600\n",
      "Created a chunk of size 1022, which is longer than the specified 600\n",
      "Created a chunk of size 1260, which is longer than the specified 600\n",
      "Created a chunk of size 795, which is longer than the specified 600\n",
      "Created a chunk of size 1293, which is longer than the specified 600\n",
      "Created a chunk of size 649, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG(Retrieval Augmented Generation)\n",
    "# your own data + general data = prompt => model\n",
    "\n",
    "# Retrival : Langchain module\n",
    "# Retriving data course : source data => load data => split data (transform) / => embed (talk about later) => store\n",
    "# loader : extract data from source and give them to langchain\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# UnstructuredFileLoader can load any type of file\n",
    "loader = UnstructuredFileLoader(\"./files/chapter_one.txt\")\n",
    "\n",
    "\n",
    "# It's too big file, we need to split out. there is 2 ways\n",
    "# 1.\n",
    "# docs = loader.load()\n",
    "# splitter.split_documents(docs)\n",
    "# 2.\n",
    "loader.load_and_split(text_splitter=splitter)\n",
    "len(loader.load_and_split(text_splitter=splitter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8eb4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d58a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
